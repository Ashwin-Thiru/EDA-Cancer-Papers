{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11748658,"sourceType":"datasetVersion","datasetId":7375511}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nHey! This is my very first notebook in my journey of exploring data science.\n\nIn this notebook, I dive into Exploratory Data Analysis (EDA)-a crucial first step in any data science or analytics project. Through hands-on experimentation and by applying techniques and ideas gathered from various sources (including Kaggle!), I’ve tried to understand my dataset, uncover patterns, and learn by doing. I hope this notebook is both helpful and fun to read!\n\n**What is EDA?**\n\nExploratory Data Analysis (EDA) is all about exploring and examining a dataset to understand its features, structure, and relationships. It helps summarize the main characteristics of the data, identify distributions, spot patterns, and detect anomalies or outliers. EDA often uses data visualization (like histograms and box plots) to make insights clearer and easier to explain.\n\n**Why is EDA important?**\n\nEDA gives data scientists a clear picture of the data, including its structure, missing values, and overall quality. Most importantly, it helps discover hidden patterns and relationships, which are essential for identifying trends, generating insights, and guiding further analysis or modeling decisions.\n\nLet’s get started and see what we can learn from the data!\n\n\n**About the DataSet**\n\nThe dataset used in this notebook consists of research documents related to cancer, specifically focused on three types: Thyroid, Lung, and Colon Cancer. As someone passionate about healthcare and medical data, I chose this dataset to begin my data science journey. It offers a rich opportunity to explore real-world clinical literature and apply data analysis techniques.\n\n**Dataset Overview:**\n\n*Rows:* 900 \n*Features:* 3 main columns-Title, Abstract, and Label \n*Labels:* Each document is categorized as Thyroid, Lung, or Colon Cancer, sourced from various medical and research repositories.\n\nThis dataset provides a mix of scientific titles and abstracts, making it ideal for practicing exploratory data analysis (EDA) in the context of natural language processing and healthcare research. Throughout this notebook, I focus on EDA-exploring the structure, key characteristics, and patterns within the data. The insights gained here will lay the foundation for any future modeling or deeper analysis.\n\n**Note:** EDA is a crucial step in understanding and preparing data for further analysis, especially in complex fields like cancer research, where uncovering patterns and relationships can lead to valuable clinical insights","metadata":{}},{"cell_type":"markdown","source":"# Data Loading & Initial Exploration","metadata":{}},{"cell_type":"code","source":"import pandas as pd  # import necessary libraries for EDA\nimport numpy as np\nimport string\nstring.punctuation\nimport re\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.preprocessing import LabelEncoder\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:59:30.351682Z","iopub.execute_input":"2025-05-10T14:59:30.352060Z","iopub.status.idle":"2025-05-10T14:59:32.993195Z","shell.execute_reply.started":"2025-05-10T14:59:30.352032Z","shell.execute_reply":"2025-05-10T14:59:32.992098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_excel('/kaggle/input/cancer-papers-dataset/data2.xlsx')  # Read the Dataset\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:59:37.101064Z","iopub.execute_input":"2025-05-10T14:59:37.101566Z","iopub.status.idle":"2025-05-10T14:59:37.972485Z","shell.execute_reply.started":"2025-05-10T14:59:37.101534Z","shell.execute_reply":"2025-05-10T14:59:37.971660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:59:41.131299Z","iopub.execute_input":"2025-05-10T14:59:41.131739Z","iopub.status.idle":"2025-05-10T14:59:41.142129Z","shell.execute_reply.started":"2025-05-10T14:59:41.131710Z","shell.execute_reply":"2025-05-10T14:59:41.141099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns  # Check for columns (features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:59:44.539805Z","iopub.execute_input":"2025-05-10T14:59:44.540164Z","iopub.status.idle":"2025-05-10T14:59:44.547782Z","shell.execute_reply.started":"2025-05-10T14:59:44.540137Z","shell.execute_reply":"2025-05-10T14:59:44.546707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()  # Basic info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:59:46.819689Z","iopub.execute_input":"2025-05-10T14:59:46.820041Z","iopub.status.idle":"2025-05-10T14:59:46.843329Z","shell.execute_reply.started":"2025-05-10T14:59:46.820017Z","shell.execute_reply":"2025-05-10T14:59:46.842323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.isnull().sum())  # Check for Null Values\nprint(\"Number of duplicate rows:\", df.duplicated().sum()) # Check for duplicates","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:59:49.750072Z","iopub.execute_input":"2025-05-10T14:59:49.750401Z","iopub.status.idle":"2025-05-10T14:59:49.764746Z","shell.execute_reply.started":"2025-05-10T14:59:49.750377Z","shell.execute_reply":"2025-05-10T14:59:49.763575Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Initial Exploration & Data Quality Overview**\n\nFrom our initial exploration, we found that this dataset is entirely text-based:\n\n* Two out of three features (Title and Abstract) are textual, making the dataset predominantly driven by text length and content.\n* The third feature (Label) is categorical, indicating the cancer type for each document.\n\nA quick check revealed that the dataset is clean and well-structured:\n* No missing (null) values in any of the features.\n* No duplicate entries detected.\n\nWith these checks complete, we can confidently proceed to more detailed *Data Quality Checks* and begin our exploratory data analysis.","metadata":{}},{"cell_type":"markdown","source":"**C. Feature Exploration**\n\nIn feature exploration, we begin by examining the class distribution using a bar graph to visualize the number of documents in each class of the Label feature (i.e., Thyroid, Colon, and Lung Cancers). Next, we explore the Title and Abstract features to better understand the content and prepare for further analysis.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = df['Label'].nunique()\nprint(\"Number of unique classes:\", num_classes)  # Number of unique classes in label\n\nunique_classes = df['Label'].unique()\nprint(\"\\nUnique classes:\", unique_classes)  # Unique class names in label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:59:59.485130Z","iopub.execute_input":"2025-05-10T14:59:59.485451Z","iopub.status.idle":"2025-05-10T14:59:59.494172Z","shell.execute_reply.started":"2025-05-10T14:59:59.485427Z","shell.execute_reply":"2025-05-10T14:59:59.492319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Label'].value_counts().plot(kind='bar', title='Label Distribution')  # Class Distribution using bar graph (count of each classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:02.019750Z","iopub.execute_input":"2025-05-10T15:00:02.020261Z","iopub.status.idle":"2025-05-10T15:00:02.428009Z","shell.execute_reply.started":"2025-05-10T15:00:02.020232Z","shell.execute_reply":"2025-05-10T15:00:02.426591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for index, text in enumerate(df['Title'][35:38]):\n    print('Title %d:\\n'%(index+1), text)  # review few samples titles from dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:06.277091Z","iopub.execute_input":"2025-05-10T15:00:06.277405Z","iopub.status.idle":"2025-05-10T15:00:06.283748Z","shell.execute_reply.started":"2025-05-10T15:00:06.277383Z","shell.execute_reply":"2025-05-10T15:00:06.282650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for index, text in enumerate(df['Abstract'][100:101]):\n    print('Abstract %d:\\n'%(index+1), text)  # review of a sample abstract from dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:09.464163Z","iopub.execute_input":"2025-05-10T15:00:09.464465Z","iopub.status.idle":"2025-05-10T15:00:09.470405Z","shell.execute_reply.started":"2025-05-10T15:00:09.464443Z","shell.execute_reply":"2025-05-10T15:00:09.469463Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Observations from Feature Exploration\n\nThe label feature, which is categorical, contains three classes: Thyroid_Cancer, Lung_Cancer, and Colon_Cancer. The dataset is evenly distributed across these classes, with each class containing 300 documents, as shown in the label distribution bar graph. Additionally, by reviewing printed samples, it is evident that the abstract feature is significantly longer than the title feature. This difference in length may contribute to the presence of outliers in the data.","metadata":{}},{"cell_type":"markdown","source":"# Text Processing\n\nText processing is a crucial step when working with text-driven data. This phase helps analyze large volumes of information by removing irrelevant content and noise, ultimately cleaning the dataset and improving its quality. Effective text processing is essential for further analysis and model training.\n\nIn this section, we will focus on cleaning the text. Before cleaning, we will combine the Title and Abstract features into a new feature called Combined_document (a common feature engineering approach for handling large textual datasets). We will then clean this combined data by converting all text to lowercase, removing punctuation, and eliminating extra spaces.\n\nCleaning the text in this way reduces noise-punctuation can introduce unnecessary complexity, and lowercasing helps normalize the data, ensuring that words are treated consistently regardless of their original capitalization.\n\nlet's get started with text processing!","metadata":{}},{"cell_type":"code","source":"df['Combined_document'] = df['Title'] + \" [SEP] \" + df['Abstract']  # Title & Abstract is merged into Combined_document (acts as a single document)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:15.851503Z","iopub.execute_input":"2025-05-10T15:00:15.851835Z","iopub.status.idle":"2025-05-10T15:00:15.871195Z","shell.execute_reply.started":"2025-05-10T15:00:15.851811Z","shell.execute_reply":"2025-05-10T15:00:15.870230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Combined_document'] = df['Combined_document'].apply(lambda x: x.lower())  # Lowercasing\ndf['Combined_document'] = df['Combined_document'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))  # removes all punctuation characters\ndf['Combined_document'] = df['Combined_document'].apply(lambda x: re.sub(' +', ' ',x))  # removes extra spacing\n\nfor index, text in enumerate(df['Combined_document'][0:1]):\n    print(\"Document %d:\\n\"%(index+1), text)  # review sample document (title+abstract)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:18.746696Z","iopub.execute_input":"2025-05-10T15:00:18.747086Z","iopub.status.idle":"2025-05-10T15:00:18.908367Z","shell.execute_reply.started":"2025-05-10T15:00:18.747061Z","shell.execute_reply":"2025-05-10T15:00:18.907324Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Document Length Analysis\n\nDocument length analysis involves measuring the length of documents-typically in words or characters-to gain insights into your dataset.\n\n**Why Analyze Document Length?**\n\n* *Understanding Complexity:* Longer documents may indicate greater complexity and require more effort to process.\n\n* *Identifying Patterns:* Analyzing document lengths can reveal patterns, such as certain topics or classes having longer or shorter texts.\n\n* *Feature Engineering:* Document length can serve as a useful feature in machine learning models.\n\n**Methods and Techniques**\n\n* *Statistical Analysis:* Use descriptive statistics (mean, median, standard deviation) to summarize document lengths.\n\n* *Visualization:* Histograms and box plots help visualize the distribution and spot outliers.","metadata":{}},{"cell_type":"code","source":"df['doc_length'] = df['Combined_document'].apply(lambda x: len(str(x).split()))\nprint(df['doc_length'].describe())\n\nplt.figure(figsize=(8, 5))\nplt.hist(df['doc_length'], bins=30, color='mediumpurple', edgecolor='black', alpha=0.8)\nplt.title('Document Length Distribution')\nplt.xlabel('Number of Words')\nplt.ylabel('Number of Documents')\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n\n# The distribution (histogram) is roughly bell-shaped (close to normal) but slightly right-skewed, with a long tail of longer documents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:22.179921Z","iopub.execute_input":"2025-05-10T15:00:22.180221Z","iopub.status.idle":"2025-05-10T15:00:22.442898Z","shell.execute_reply.started":"2025-05-10T15:00:22.180196Z","shell.execute_reply":"2025-05-10T15:00:22.441830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nsns.boxplot(x='Label', y='doc_length', data=df, palette='Set2')\nplt.title('Class-wise Document Length Distribution')\nplt.xlabel('Cancer Type')\nplt.ylabel('Document Length (words)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:25.709851Z","iopub.execute_input":"2025-05-10T15:00:25.710231Z","iopub.status.idle":"2025-05-10T15:00:26.046986Z","shell.execute_reply.started":"2025-05-10T15:00:25.710207Z","shell.execute_reply":"2025-05-10T15:00:26.045651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"avg_lengths = df.groupby('Label')['doc_length'].mean()\nprint(\"Average Document Length by Class:\")\nprint(avg_lengths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:28.340380Z","iopub.execute_input":"2025-05-10T15:00:28.340735Z","iopub.status.idle":"2025-05-10T15:00:28.352723Z","shell.execute_reply.started":"2025-05-10T15:00:28.340710Z","shell.execute_reply":"2025-05-10T15:00:28.351485Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Observation on Document Length Analysis\n\n* The document length distribution is approximately normal, with most documents containing between 150 and 350 words.\n* All three cancer types have similar document length distributions, with median lengths around 250 words and comparable variability.\n* Average document lengths are close across classes (Colon: 245, Thyroid: 253, Lung: 262 words).\n* Outliers are present in all classes, especially among Colon_Cancer documents, which include *a few very long* entries.\n* These findings suggest the dataset is well-balanced in terms of document length, but attention should be paid to outliers in further analysis.","metadata":{}},{"cell_type":"markdown","source":"# Word Frequency Analysis\n\nWord frequency analysis is a foundational technique in qualitative text analysis. It involves counting how often each word or phrase appears within a text or a collection of documents. This method helps identify the most prominent topics, recurring themes, or key terms in the dataset.\n\nWhile word frequency analysis highlights which words are most common-shedding light on the internal focus and dominant subjects of the text-document length analysis measures how long each document is (in terms of word count). Document length can influence word frequency counts, as longer documents may naturally contain more occurrences of certain words.\n\nIn summary, word frequency analysis reveals the internal distribution and prominence of terms within your data, whereas document length analysis provides context about the size and potential variability of your documents. Together, these analyses offer valuable insights into both the content and structure of your textual dataset.","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nwords = ' '.join(df['Combined_document']).split()  # Most Frequent Words Overall\nfreq = Counter(words).most_common(30)\npd.DataFrame(freq, columns=['word', 'Frequency']).plot(kind='bar', x='word', y='Frequency', title='Top 30 Words')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:32.100415Z","iopub.execute_input":"2025-05-10T15:00:32.100740Z","iopub.status.idle":"2025-05-10T15:00:32.650636Z","shell.execute_reply.started":"2025-05-10T15:00:32.100715Z","shell.execute_reply":"2025-05-10T15:00:32.649377Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Most words in the graph are stopwords, which add little domain-specific meaning. Removing them helps highlight the true, meaningful terms relevant to the cancer dataset.","metadata":{}},{"cell_type":"code","source":"for label in df['Label'].unique():\n  words = ' '.join(df[df['Label']==label]['Combined_document']).split()\n  freq = Counter(words).most_common(10)\n  print(f\"\\nLabel: {label}\")\n  print(freq)  # Most Frequent Words by Label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:34.720723Z","iopub.execute_input":"2025-05-10T15:00:34.721758Z","iopub.status.idle":"2025-05-10T15:00:34.827231Z","shell.execute_reply.started":"2025-05-10T15:00:34.721426Z","shell.execute_reply":"2025-05-10T15:00:34.825457Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This review also contains many stopwords, so we will focus on removing them in the next phase to better capture meaningful domain-specific terms.","metadata":{}},{"cell_type":"markdown","source":"# Stop Word Impact & Removal\n\nFrom the previous two analyses, we see that stopwords dominate the top ranks in our visuals, overshadowing important domain-specific terms. Therefore, we will now focus on removing stopwords to better highlight the meaningful words in our dataset.\n\n*Stop word removal* is crucial in Exploratory Data Analysis (EDA) of text data because it reduces noise and focuses on meaningful words, improving analysis accuracy and model performance. By removing common, uninformative words like \"a\", \"the\", and \"is,\" EDA can identify key themes, topics, and relationships within the text more effectively.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nstop_words = ENGLISH_STOP_WORDS\n\ndef remove_stopwords(text):\n    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n\ndf['Cleaned_document'] = df['Combined_document'].apply(remove_stopwords)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:37.609845Z","iopub.execute_input":"2025-05-10T15:00:37.610180Z","iopub.status.idle":"2025-05-10T15:00:37.681213Z","shell.execute_reply.started":"2025-05-10T15:00:37.610158Z","shell.execute_reply":"2025-05-10T15:00:37.679337Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, re-run the previous visual, and check for new observations. Ww should see a much clearer representation of the key topics and terms in related to cancer dataset!","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nwords = ' '.join(df['Cleaned_document']).split()  # Most Frequent Words Overall\nfreq = Counter(words).most_common(30)\npd.DataFrame(freq, columns=['word', 'Frequency']).plot(kind='bar', x='word', y='Frequency', title='Top 30 Words')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:40.359674Z","iopub.execute_input":"2025-05-10T15:00:40.360021Z","iopub.status.idle":"2025-05-10T15:00:40.839169Z","shell.execute_reply.started":"2025-05-10T15:00:40.359997Z","shell.execute_reply":"2025-05-10T15:00:40.837923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for label in df['Label'].unique():\n  words = ' '.join(df[df['Label']==label]['Cleaned_document']).split()\n  freq = Counter(words).most_common(10)\n  print(f\"\\nLabel: {label}\")\n  print(freq)  # Most Frequent Words by Label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:44.346448Z","iopub.execute_input":"2025-05-10T15:00:44.347320Z","iopub.status.idle":"2025-05-10T15:00:44.408750Z","shell.execute_reply.started":"2025-05-10T15:00:44.347290Z","shell.execute_reply":"2025-05-10T15:00:44.407008Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Yes, as we can see, domain-specific words now dominate the visuals. Thanks to the stopword removal function!","metadata":{}},{"cell_type":"markdown","source":"# Word Cloud Visualizations\n\nWord clouds are a valuable technique in Exploratory Data Analysis (EDA), especially when working with text data. They provide a visual representation of word frequency, helping to identify key themes and insights within the text.\n\nHere, we use word clouds to visualize both the overall words throughout the dataset and the words specific to each class.","metadata":{}},{"cell_type":"code","source":"text = ' '.join(df['Cleaned_document'])\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Overall Word Cloud\")  # Word Cloud (Visual)\nplt.show() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:47.540381Z","iopub.execute_input":"2025-05-10T15:00:47.540696Z","iopub.status.idle":"2025-05-10T15:00:49.943899Z","shell.execute_reply.started":"2025-05-10T15:00:47.540672Z","shell.execute_reply":"2025-05-10T15:00:49.942584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Class-wise WordCloud with extra space between plots\nfor label in df['Label'].unique():\n    plt.figure(figsize=(10,5))\n    text = ' '.join(df[df['Label']==label]['Cleaned_document'])\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title(f\"Word Cloud for {label}\")\n    plt.tight_layout(pad=3)  # Adds extra padding around the plot\n    plt.show()\n    plt.close()  # Ensures plots don't overlap in some environments\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:00:55.330475Z","iopub.execute_input":"2025-05-10T15:00:55.330907Z","iopub.status.idle":"2025-05-10T15:01:00.845108Z","shell.execute_reply.started":"2025-05-10T15:00:55.330861Z","shell.execute_reply":"2025-05-10T15:01:00.843771Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we can see, the word clouds look great, with domain-specific words prominently featured in both the overall and class-wise visualizations.","metadata":{}},{"cell_type":"markdown","source":"# Feature Importance \n\nFeature importance measures how much each feature (such as a word or variable) influences a machine learning model’s predictions. In text analysis, feature importance *(often using TF-IDF)* highlights words that are not just frequent but are also *distinctive* and *informative* for classification or prediction tasks.\n\n**Difference from frequency word analysis:**\n\n* *Word frequency analysis* simply counts how often each word appears, showing the most common terms but not their usefulness for distinguishing between classes.\n\n* *Feature importance (e.g., via TF-IDF)* identifies which words are most valuable for prediction by considering both their frequency and their uniqueness across documents, making it more effective for model building and interpretation.\n\n**In short:**\n\n* *Word frequency:* Most common words.\n* *Feature importance:* Most influential words for the model’s decisions.","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df['Cleaned_document'])  # TF-IDF Feature Importance (Top Words)\ntfidf_scores = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\ntop_words = tfidf_scores.sum().sort_values(ascending=False).head(30)\ntop_words.plot(kind='bar', title='Top TF-IDF Words') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:01:07.112042Z","iopub.execute_input":"2025-05-10T15:01:07.112399Z","iopub.status.idle":"2025-05-10T15:01:07.986020Z","shell.execute_reply.started":"2025-05-10T15:01:07.112378Z","shell.execute_reply":"2025-05-10T15:01:07.985057Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Observation from Feature Importance\n\nFrequent word analysis highlights the most common terms, while feature importance emphasizes words that carry more meaning or predictive power. Comparing both reveals that some words may be frequent but not necessarily important, and vice versa.","metadata":{}},{"cell_type":"markdown","source":"# Correlation Analysis\n\nCorrelation analysis measures how strongly two variables are related to each other. In the context of text data, when we select the top TF-IDF features (the most important words or terms across documents), correlation analysis examines how the presence or importance of one term relates to another across the entire dataset.\n\nBy calculating the correlation between top TF-IDF features, we can identify which terms tend to occur together (positive correlation) or rarely appear together (negative correlation) across documents.\n\nA heatmap visually displays these correlations, making it easier to spot patterns, clusters, or redundancies among features.\n\nHighly correlated features may indicate redundancy, which can be useful for feature selection or dimensionality reduction. It also helps in understanding relationships between key terms, which can enhance tasks like topic modeling or document classification.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\ntfidf = TfidfVectorizer(max_features=50, stop_words='english')\nX_tfidf = tfidf.fit_transform(df['Cleaned_document'])\ntfidf_df = pd.DataFrame(X_tfidf.toarray(),\n                        columns=tfidf.get_feature_names_out())\n\n# Correlation Matrix among features\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Compute correlation matrix\ncorr_matrix = tfidf_df.corr()\n\n# Plot heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix, cmap='coolwarm', center=0)\nplt.title('Correlation Matrix of Top TF-IDF Features')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:01:12.820985Z","iopub.execute_input":"2025-05-10T15:01:12.821295Z","iopub.status.idle":"2025-05-10T15:01:13.894794Z","shell.execute_reply.started":"2025-05-10T15:01:12.821276Z","shell.execute_reply":"2025-05-10T15:01:13.893781Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Observations from Correlation Heatmap of Top TF-IDF Features\n\n* Most TF-IDF features show low correlation with each other, indicating feature independence.\n* The diagonal displays perfect correlation (value = 1) as expected.\n* A few small clusters of moderate correlation exist, reflecting related domain-specific terms that often co-occur.\n* There is no large block of high correlation, suggesting minimal redundancy among features.\n* The features capture diverse and meaningful information from the text data.\n* Low multicollinearity implies that classification models can effectively leverage these features without risk of overfitting due to redundant inputs.\n* The heatmap confirms that the feature selection and preprocessing steps were successful in extracting relevant and distinct textual features.\n* Small correlated groups may highlight interesting semantic or topical relationships worth exploring further.","metadata":{}},{"cell_type":"markdown","source":"**Encode Labels Numerically**\n\nLabel encoding is useful in EDA because it converts categorical labels into numeric form, enabling easier analysis, visualization, and statistical summarization of the target variable. This numeric representation also prepares the data for machine learning models, which require numerical inputs to process and learn effectively","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()  # Encode Labels Numerically\ndf['label_num'] = le.fit_transform(df['Label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:01:19.641421Z","iopub.execute_input":"2025-05-10T15:01:19.641859Z","iopub.status.idle":"2025-05-10T15:01:19.648670Z","shell.execute_reply.started":"2025-05-10T15:01:19.641831Z","shell.execute_reply":"2025-05-10T15:01:19.647260Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Feature-Label(target) Correlation Analysis**\n\nFeature-label correlation in EDA helps pinpoint the words most associated with each label, guiding both data understanding and the selection of meaningful features for downstream modeling.","metadata":{}},{"cell_type":"code","source":"# Feature-Label Correlation\n# Add label to tfidf_df for correlation\ntfidf_df['label_num'] = df['label_num']\n\n# Correlation of each feature with the label\nfeature_label_corr = tfidf_df.corr()['label_num'].drop('label_num').sort_values(ascending=False)\nprint(\"Top positively correlated words with label:\\n\", feature_label_corr.head(10))\nprint(\"\\nTop negatively correlated words with label:\\n\", feature_label_corr.tail(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:03:29.954647Z","iopub.execute_input":"2025-05-10T15:03:29.955091Z","iopub.status.idle":"2025-05-10T15:03:29.979155Z","shell.execute_reply.started":"2025-05-10T15:03:29.955064Z","shell.execute_reply":"2025-05-10T15:03:29.977994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Observation from Correlation Analysis (Feature-Label Correlation)\n\n**Strongest positive correlation:**\n\n* thyroid (0.72) and ptc (0.34) are highly positively correlated with the label, indicating these words are strong indicators of a particular class.\n\n**Strongest negative correlation:**\n\n* colon (-0.72) and cancer (-0.19) are highly negatively correlated, suggesting these words are strong indicators of a different class.\n* Other words (like patients, risk, therapy, outcomes, cells, etc.) have moderate positive or negative correlations, indicating their varying importance in distinguishing between classes.\n\nI'm skipping Dimensionality reduction (PCA)","metadata":{}},{"cell_type":"markdown","source":"# Outlier Detection \n\nHere, Outlier detection helps identify documents that are unusually short, long, or have abnormal TF-IDF sums. These documents may be errors, irrelevant, or unrepresentative, and can negatively impact model performance and data quality.\n\nWe calculate the total TF-IDF score for each document and use the IQR (Interquartile Range) method to find documents with abnormally high or low TF-IDF sums.\n\nWe compute the IQR for document lengths and identify documents that are much shorter or longer.","metadata":{}},{"cell_type":"code","source":"tfidf_sums = X_tfidf.sum(axis=1).A1  # Sum TF-IDF values for each document\n\nQ1 = np.percentile(tfidf_sums, 25)  # Calculate IQR for document TF-IDF sums\nQ3 = np.percentile(tfidf_sums, 75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\noutlier_docs = np.where((tfidf_sums < lower_bound) | (tfidf_sums > upper_bound))[0]  # Identify documents with outlier TF-IDF sums\nprint(f\"Number of documents with outlier TF-IDF sums: {len(outlier_docs)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:21:51.221390Z","iopub.execute_input":"2025-05-10T15:21:51.221840Z","iopub.status.idle":"2025-05-10T15:21:51.232009Z","shell.execute_reply.started":"2025-05-10T15:21:51.221811Z","shell.execute_reply":"2025-05-10T15:21:51.230685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Q1 = df['doc_length'].quantile(0.25)\nQ3 = df['doc_length'].quantile(0.75)\nIQR = Q3 - Q1\n\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\nprint(f\"Lower bound for outliers: {lower_bound}\")\nprint(f\"Upper bound for outliers: {upper_bound}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:22:07.729403Z","iopub.execute_input":"2025-05-10T15:22:07.729745Z","iopub.status.idle":"2025-05-10T15:22:07.739443Z","shell.execute_reply.started":"2025-05-10T15:22:07.729723Z","shell.execute_reply":"2025-05-10T15:22:07.738372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"short_outliers = df[df['doc_length'] < 72.0]\nlong_outliers = df[df['doc_length'] > 440.0]\n\nprint(f\"Number of too short documents: {len(short_outliers)}\")\nprint(f\"Number of too long documents: {len(long_outliers)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:23:06.719549Z","iopub.execute_input":"2025-05-10T15:23:06.719917Z","iopub.status.idle":"2025-05-10T15:23:06.729428Z","shell.execute_reply.started":"2025-05-10T15:23:06.719868Z","shell.execute_reply":"2025-05-10T15:23:06.728424Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Observation from Outlier Analysis\n\nOut of all documents, only 4 have outlier TF-IDF sums, indicating that most documents have typical content richness. However, 52 documents are unusually short and 20 are unusually long based on document length, suggesting some variability in document size within the dataset.\n\nMost documents fall within normal ranges for both TF-IDF content and length, but a small number of documents are flagged as outliers. \n\nWell, we can flag those outlier doucments, keep it for future review.","metadata":{}},{"cell_type":"markdown","source":"# Final EDA Conclusion Report\n\n**Key Findings and Observations**\n\n*Data Quality:* The dataset is generally clean, with most documents falling within normal ranges for both TF-IDF content and document length. Only a small fraction of documents were identified as outliers, which have been flagged for further review.\n\n*Feature Independence:* Correlation analysis among the top 50 TF-IDF features shows low multicollinearity, indicating that features are mostly independent and suitable for modeling.\n\n*Label-Feature Relationships:* Several words, such as thyroid and colon, exhibit strong positive or negative correlations with the target label, providing valuable insights into the most influential terms for classification.\n\n*Class Distribution:* The label encoding process revealed a balanced class distribution, with no significant class imbalance detected.\n\n*Document Length Variability:* While most documents are of typical length, a subset is much shorter or longer, which may warrant further inspection for data consistency.\n\n**Valuable Insights**\n\n* The most predictive words for each class have been identified, supporting both interpretability and targeted feature engineering.\n* Outlier detection ensures that data quality is maintained, reducing the risk of noise or bias in downstream modeling.\n* The feature set is well-prepared for machine learning, with minimal redundancy and strong interpretability.\n\n**Potential Next Steps**\n\n*Feature Engineering:*\n* Explore n-grams or domain-specific keywords to enhance predictive power.\n* Consider dimensionality reduction if further simplification is needed.\n\n*Modeling:*\n* Proceed with classification models (e.g., logistic regression, random forest, SVM) using the current feature set.\n* Evaluate model performance and refine features based on feature importance and validation results.\n\n*Outlier Review:*\n* Manually review flagged outlier documents to decide on removal or retention.\n* Document any changes made for transparency and reproducibility.","metadata":{}},{"cell_type":"markdown","source":"# My Learnings\n\nThe EDA process has provided a comprehensive understanding of the dataset’s structure, quality, and key features. The data is well-prepared for modeling, with clear insights into the most influential terms and minimal redundancy. With outliers flagged and features validated, the project is ready to move confidently into the next phase.\n\nIt was a great learning experience, I'll come up with few more notebooks in future. Share my learnings and findings. \n\nThankyou for reading till here, Hope you liked it :)","metadata":{}}]}